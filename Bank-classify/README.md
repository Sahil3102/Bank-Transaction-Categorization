# Bank Transaction Categorization

## 1. Project Overview

This project is a machine learning pipeline designed to automatically classify bank transaction descriptions into a set of predefined categories. By leveraging Natural Language Processing (NLP) techniques and classic machine learning models, it provides an automated way to understand and organize financial data.

The project is structured to follow best practices in machine learning engineering, with separate modules for data processing, model training, and a user-facing application.

**Categories:** Food, Travel, Salary, Rent, Shopping, Transfer, Others.

## 2. Project Structure

```
├── app/
│   ├── app.py
│   └── model.joblib
├── data/
│   ├── transactions.csv
│   └── data_generator.py
├── notebooks/
│   └── EDA_and_Model.ipynb
├── src/
│   ├── preprocessing.py
│   └── model.py
├── requirements.txt
└── README.md
```

## 3. Dataset

The dataset is synthetically generated to mimic real-world bank transactions. It contains the following columns:

- **Date**: The date and time of the transaction.
- **Description**: A short text describing the transaction.
- **Amount**: The transaction amount.
- **Category**: The ground truth category for the transaction.

If `data/transactions.csv` is not present, it can be generated by running the `data/data_generator.py` script.

## 4. Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`
    ```

3.  **Install the required dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Download NLTK data:**
    The first time you run the preprocessing module (either via the notebook or the app), it will download the necessary NLTK `stopwords` and `wordnet` packages.

## 5. How to Run

### 5.1. Jupyter Notebook (EDA and Model Training)

The `notebooks/EDA_and_Model.ipynb` notebook provides a detailed walkthrough of the project, from data loading to model evaluation.

1.  **Start Jupyter Lab or Jupyter Notebook:**
    ```bash
    jupyter lab
    ```
    or
    ```bash
    jupyter notebook
    ```

2.  Navigate to the `notebooks` directory and open `EDA_and_Model.ipynb`.

3.  Run the cells sequentially to see the data analysis, model comparison, and to save the best-performing model to the `app/` directory.

### 5.2. Streamlit Application

The Streamlit app allows for real-time categorization of new transaction data.

1.  **Ensure a trained model exists.** Run the Jupyter notebook first to generate and save `app/model.joblib`.

2.  **Run the Streamlit app from the root directory:**
    ```bash
    streamlit run app/app.py
    ```

3.  The application will open in your web browser. You can upload a CSV file containing `Description` and `Amount` columns to see the categorized results.

## 6. Sample Outputs

### Notebook: Class Distribution
*(Add a screenshot of the class distribution bar plot from the notebook here)*

### Streamlit App: Categorized Output
*(Add a screenshot of the Streamlit app showing the categorized transaction data here)* 